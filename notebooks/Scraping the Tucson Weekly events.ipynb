{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping in the mire\n",
    "\n",
    "I wanted to write a fairly simple web app that would provide a \"value added\" search of Tucson event listings in the Tucson Weekly. A simple search of events occuring over a weekend returns hundreds of results to potentially sift through, and personally I'd like to filter out certain things like weekly drum circles and listings of an ongoing months-long exhibition in a generic art gallery.\n",
    "\n",
    "Obviously the website allows you to do some simple filtering, but I'm going to see if I can tune it up.\n",
    "\n",
    "There is this great [blog post](http://nycdatascience.com/nyc-concert-data-web-scraping-with-python/) that demos exactly how to start this kind of project using BeautifulSoup via an example of scrapting the concert listings from NYC's Village Voice.\n",
    "\n",
    "## First task: simple scrape of event data into a table\n",
    "\n",
    "Later on, when this is approaching an initial version of the actual app, this will be turned into a weekly scheduled job that grabs the event data from the current date until some future date (maybe a month?) and stores it.\n",
    "\n",
    "Crawling over the pages returned for a particular date, starting from the current date, and going up to some N days later is straight forward. All that was needed was minor modifications to the python code from the blog post. \n",
    "\n",
    "However, unlike the Village Voice music listings the html `div` tags used by the Tucson Weekly website are often a lot more obscure (in my html-newbie opinion). After sifting through the html I found that the `div` tag labelled \"EventListing clearfix\" was the one that corresponding to a single event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE:\n",
      "2016-02-07\n",
      "On page = 1\n",
      "There are 15 events on this page\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    " \n",
    "# just do current day for illustration\n",
    "numDays = 1\n",
    "for j in range(0, numDays):\n",
    "\n",
    "    nEvents = 0\n",
    "    \n",
    "    # get string with date\n",
    "    date = datetime.date.today() + datetime.timedelta(days = j) \n",
    "    dateString = date.strftime(\"%Y-%m-%d\")\n",
    "    print \"DATE:\"\n",
    "    print dateString\n",
    "    \n",
    "    \n",
    "    baseUrl = 'http://www.tucsonweekly.com/tucson/EventSearch?narrowByDate='\n",
    "    \n",
    "    # while loop to crawl over pages\n",
    "    # Set maximum possible number of pages of events: \n",
    "    # in practice would be a huge number, and the code should break before this\n",
    "    npageMax = 2 # for illustration, we'll stop after one page\n",
    "\n",
    "    \n",
    "    # base URL for this date\n",
    "    url = baseUrl + dateString\n",
    "    ii=0\n",
    "    for pageNum in xrange(1, npageMax):\n",
    "        print \"On page =\", pageNum\n",
    "    \n",
    "        # crawl over pages via appending initial date url\n",
    "        if (pageNum>1):\n",
    "            url = baseUrl + dateString + \"&page=\" + str(pageNum)\n",
    "        \n",
    "        # parse webpage with BeautifulSoup\n",
    "        text = requests.get(url).text  # get raw html\n",
    "        soup = BeautifulSoup(text)     # input it into BeautifulSoup to parse it\n",
    "        \n",
    "        # get all \"div\" tags that corresond to a single event\n",
    "        eventTags = soup.find_all('div', 'EventListing clearfix')\n",
    "        print \"There are\", len(eventTags), \"events on this page\"\n",
    "        \n",
    "        nEvents += len(eventTags)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we've found the events, we need to grab their data!\n",
    "\n",
    "And this is where it got more irritating. Within each: \n",
    "\n",
    "    <div class=\"EventListing clearfix\" ...\n",
    "    \n",
    "I needed to find the url of the event, and unfortunately unlike the Village Voice, Tucson Weekly wasn't nice in just allowing me to grab all anchor tags with the attribute `href` like this:\n",
    "\n",
    "    artistLinks = [tag.a.attrs['href'] for tag in artistTags]\n",
    "    \n",
    "Each event had multiple anchor tags, many with an `href` attribute that was not the url I was after. \n",
    "\n",
    "I managed to find the following fix: iterate over each anchor tag looking for the first one that has a `href` attribute, but does NOT have a `class` attribute. This seems to return the correct anchor tag with the `href` attribute giving the event's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT 0 START:\n",
      "Event name = 43rd Annual President’s Concert\n",
      "Event link = http://www.tucsonweekly.com/tucson/43rd-annual-presidents-concert/Event?oid=6015139\n",
      "EVENT 1 START:\n",
      "Event name = Special Liturgical Music: Arvo Pärt's Berliner Messe\n",
      "Event link = http://www.tucsonweekly.com/tucson/special-liturgical-music-arvo-parts-berliner-messe/Event?oid=6022644\n",
      "EVENT 2 START:\n",
      "Event name = How to Live in Happiness\n",
      "Event link = http://www.tucsonweekly.com/tucson/how-to-live-in-happiness/Event?oid=6021947\n",
      "EVENT 3 START:\n",
      "Event name = Big Game Viewing Party\n",
      "Event link = http://www.tucsonweekly.com/tucson/big-game-viewing-party/Event?oid=6025673\n",
      "EVENT 4 START:\n",
      "Event name = Community Forum for LGBT Seniors and Friends\n",
      "Event link = http://www.tucsonweekly.com/tucson/community-forum-for-lgbt-seniors-and-friends/Event?oid=6021664\n",
      "EVENT 5 START:\n",
      "Event name = Reel in the Closet\n",
      "Event link = http://www.tucsonweekly.com/tucson/reel-in-the-closet/Event?oid=6020267\n",
      "EVENT 6 START:\n",
      "Event name = Your Wellness Journey-WellWays Workshop\n",
      "Event link = http://www.tucsonweekly.com/tucson/your-wellness-journey-wellways-workshop/Event?oid=6019458\n",
      "EVENT 7 START:\n",
      "Event name = Tucson Ukulele Meetup\n",
      "Event link = http://www.tucsonweekly.com/tucson/tucson-ukulele-meetup/Event?oid=6017963\n",
      "EVENT 8 START:\n",
      "Event name = Ron DeVous\n",
      "Event link = http://www.tucsonweekly.com/tucson/ron-devous/Event?oid=6022296\n",
      "EVENT 9 START:\n",
      "Event name = Ron Doering & RonDeVous Revue\n",
      "Event link = http://www.tucsonweekly.com/tucson/ron-doering-and-rondevous-revue/Event?oid=6014090\n",
      "EVENT 10 START:\n",
      "Event name = Art Walk Sundays\n",
      "Event link = http://www.tucsonweekly.com/tucson/art-walk-sundays/Event?oid=6008150\n",
      "EVENT 11 START:\n",
      "Event name = Art & Crafts Festival\n",
      "Event link = http://www.tucsonweekly.com/tucson/art-and-crafts-festival/Event?oid=6015383\n",
      "EVENT 12 START:\n",
      "Event name = Rhythms of the Americas\n",
      "Event link = http://www.tucsonweekly.com/tucson/rhythms-of-the-americas/Event?oid=5998815\n",
      "EVENT 13 START:\n",
      "Event name = Sorne - live performances & vocal workshop\n",
      "Event link = http://www.tucsonweekly.com/tucson/sorne-live-performances-and-vocal-workshop/Event?oid=6011617\n",
      "EVENT 14 START:\n",
      "Event name = \"Desert Schemes\"\n",
      "Event link = http://www.tucsonweekly.com/tucson/desert-schemes/Event?oid=5981354\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# loop over each event and attempt to extract the html link for the page of the event\n",
    "for i, event in enumerate(eventTags):\n",
    "\n",
    "    print \"EVENT\", i ,\"START:\"\n",
    "        \n",
    "    # find all the anchor ('a') tags within the 'div' tag for the evetn\n",
    "    anchor_tags = event.find_all('a')\n",
    "        \n",
    "        \n",
    "    # iterate over each anchor tag looking for the FIRST one that has a 'href' attribute\n",
    "    # but does NOT have a 'class' attribute\n",
    "    isFound = False\n",
    "    for j,mb in enumerate(anchor_tags):\n",
    "        \n",
    "        if (mb.has_attr('href') and (not isFound) and (not mb.has_attr('class')) ):\n",
    "            event_name = mb.get_text().strip() # this should be the event name! (type unicode)\n",
    "            event_link = mb.attrs['href']      # this should be the event webpage! (type string)\n",
    "            \n",
    "            print \"Event name =\", event_name\n",
    "            print \"Event link =\", event_link\n",
    "            \n",
    "            isFound = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that is left is to go to each individual event url and scrape the key information.\n",
    "\n",
    "Following the blog post I define my own `scrape` function and modify to fit with the Tucson Weekly event properties.\n",
    "\n",
    "Again this is much tougher than the Village Voice case. Not all events have the same properties, and even the ones that do have some variation between the format of the data.\n",
    "\n",
    "Also the `div` tags were not as straightforward, for example I couldn't just call:\n",
    "    \n",
    "    find('div', 'when')\n",
    "    find('div', 'price')\n",
    "    find('div', 'neighborhood')\n",
    "    \n",
    "on the BeautifulSoup object, but instead look in the meta data and in a `div` tag with `class=\"MainColumn Event\"` and `id=\"EventMetaData\"`\n",
    "\n",
    "Then it gets even more fudgey where I have to use the `span` tag, that always has `class=\"label\"` no matter the event property it has. This means I have to find the property type from the actual text covered by the `span`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event name = \"Desert Schemes\"\n",
      "Venue = Desert Artisans' Gallery\n",
      "Kind = Art \n",
      "\n",
      "Description = “Desert Schemes” new art exhibit through Feb 7th works by Margaret Aden, Gail Brynildsen, Denyse Fenelon, Pamela Howe, Tom Kolt and Jan Thompson. \n",
      "\n",
      "Address = 6536 E. Tanque Verde Road. Tucson Arizona \n",
      "Lat, lon = 32.24576 -110.85340 \n",
      "\n",
      "Price = 0.0\n",
      "\n",
      "When = First Monday-Sunday of every month, 10 a.m.-5 p.m. Continues through Feb. 7 \n",
      "\n",
      "(u'\"Desert Schemes\"', u'\\u201cDesert Schemes\\u201d new art exhibit through Feb 7th works by Margaret Aden, Gail Brynildsen, Denyse Fenelon, Pamela Howe, Tom Kolt and Jan Thompson.', '6536 E. Tanque Verde Road.TucsonArizona', 0.0, u'First Monday-Sunday of every month, 10 a.m.-5 p.m. Continues through Feb. 7')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# function to scrape info from a given link on site\n",
    "def scrape(link):\n",
    "\n",
    "    # parse webpage with BeautifulSoup\n",
    "    text = requests.get(link).text\n",
    "    soup = BeautifulSoup(text)\n",
    "    \n",
    "    # extract venue and genre(s) from <title>stuff|stuff|stuff|</title>\n",
    "    # at top of html\n",
    "    title = soup.title.string\n",
    "    title = title.split(' | ')\n",
    "    name = title[0]\n",
    "    venue = title[1]\n",
    "    genre = title[2]\n",
    "    print \"Event name =\", name\n",
    "    print \"Venue =\", venue \n",
    "    print \"Kind =\", genre ,\"\\n\"\n",
    "    \n",
    "    \n",
    "    ### Get Description\n",
    "    description = soup.find_all(\"meta\", {\"name\":\"description\"} )[0][\"content\"]\n",
    "    print \"Description =\", description ,\"\\n\"\n",
    "    \n",
    "    \n",
    "    ### Get Address stuff\n",
    "    address = [\"og:street-address\", \"og:latitude\", \"og:longitude\", \"og:locality\",\n",
    "               \"og:region\", \"og:postal-code\"]\n",
    "    addr_stuff = []\n",
    "    for ad in address:\n",
    "        result = soup.find_all(\"meta\", {\"property\": ad})\n",
    "        if (len(result)>0):\n",
    "            addr_stuff.append(result[0][\"content\"])\n",
    "        else:\n",
    "            addr_stuff.append(None)\n",
    "        \n",
    "    \n",
    "    street = addr_stuff[0]\n",
    "    lat = addr_stuff[1]\n",
    "    lon = addr_stuff[2]\n",
    "    locale = addr_stuff[3]\n",
    "    region = addr_stuff[4]\n",
    "    zipc = addr_stuff[5]\n",
    "    \n",
    "    print \"Address =\", street, locale, region, zipc\n",
    "    print \"Lat, lon =\", lat, lon ,\"\\n\"\n",
    "    \n",
    "\n",
    "    ### Get event meta data\n",
    "    metaData = soup.find_all(\"div\", {\"class\":\"MainColumn Event \", \"id\":\"EventMetaData\"} )\n",
    "    spans = metaData[0].find_all(\"span\")\n",
    "    \n",
    "    ### Look for text indicating event property within each span tag\n",
    "    foundWhen = False\n",
    "    foundPrice = False\n",
    "    textPrice = 'none'\n",
    "    for i, cont in enumerate(spans):\n",
    "\n",
    "        if (cont.text==\"When:\"):\n",
    "            when = cont.next_sibling.strip()\n",
    "            foundWhen = True\n",
    "            \n",
    "        if (cont.text==\"Price:\"):\n",
    "            textPrice = cont.next_sibling.strip()\n",
    "            foundPrice = True\n",
    "            \n",
    "\n",
    "            \n",
    "    ### Some events don't have a 'when' property, \n",
    "    #   e.g. they just say 'every 4th Friday within event description\n",
    "    if (not foundWhen):\n",
    "        print \"returning none\"\n",
    "        return None\n",
    "            \n",
    "\n",
    "    ### Parse price value\n",
    "    \n",
    "    # use regular expressions (re) to scrape multiple prices\n",
    "    # ()    look for groups of the character sequence inside the brackets\n",
    "    # \\$    look for a dollar sign at the front (needs escape character \\)\n",
    "    # \\d    look for decimal digit [0-9]\n",
    "    # +     greedy, make longest sequence possible (match one or more of the preciding RE)\n",
    "    # \\.    look for '.' (needs escape character \\)\n",
    "    # ?     Causes the resulting RE to match 0 or 1 repetitions of the preceding RE. \n",
    "    #       ab? will match either 'a' or 'ab'. i.e. doesn't HAVE to match the thing immediately proceeding it\n",
    "    \n",
    "    dollarValues = re.findall('(\\$?\\d+\\.?\\d?\\d?)', textPrice)\n",
    "    \n",
    "    # checks if price contains free \n",
    "    if (textPrice.count('free') != 0) or (textPrice.count('Free') != 0) or (textPrice.count('FREE') != 0): \n",
    "        minprice = maxprice = 0.00\n",
    "    elif len(dollarValues) == 0:\n",
    "        minprice = maxprice = None\n",
    "    else:\n",
    "        for i in range(0, len(dollarValues)):\n",
    "            dollarValues[i] = float(dollarValues[i].strip('$'))\n",
    "        minprice = min(dollarValues)\n",
    "        maxprice = max(dollarValues)\n",
    "\n",
    "        \n",
    "    if (maxprice != minprice):\n",
    "        print \"Price range =\", minprice, \"to\", maxprice\n",
    "        price = str(minprice) + \" to \" + str(maxprice)\n",
    "    else:\n",
    "        price = minprice\n",
    "        print \"Price =\", minprice\n",
    "    \n",
    "    print \"\\nWhen =\", when ,\"\\n\"\n",
    "    \n",
    "    return name, description, street + locale + region + zipc, price, when\n",
    "\n",
    "    \n",
    "# returns tuple of name, description, street + locale + region + zipc, price, when\n",
    "print scrape(event_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, we won!\n",
    "\n",
    "In long, there are still a bunch of problems:\n",
    "\n",
    "  1. the returned data is not in a consistent format\n",
    "  2. if the event has no \"When: \" text it is not included, even though the \"when\" is probably given in the event description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
